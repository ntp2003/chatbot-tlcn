{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid JSONL file\n",
      "Valid JSONL file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttps://jsonltools.com/jsonl-validator\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tone_file_path = \"fine-tuning_data/train_tone_ds1.jsonl\"\n",
    "validation_tone_file_path = \"fine-tuning_data/validation_tone_ds.jsonl\"\n",
    "def validate_jsonl(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                json.loads(line)\n",
    "        print(\"Valid JSONL file\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSONL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "\n",
    "validate_jsonl(train_tone_file_path)\n",
    "validate_jsonl(validation_tone_file_path)\n",
    "\n",
    "'''\n",
    "https://jsonltools.com/jsonl-validator\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 50\n",
      "First example:\n",
      "{'role': 'system', 'content': \"## RULES:\\n- Use only the Vietnamese language in your response. Always refer to yourself using 'em' pronoun. Address the user based on how they refer to themselves . If their preferred address term cannot be determined from their self-reference, then base it on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"}\n",
      "{'role': 'system', 'content': '## BASE KNOWLEDGE:\\n- User gender: female'}\n",
      "{'role': 'system', 'content': '## INSTRUCTIONS:\\n You must ask the user for their contact information to provide better advice on their issue or policy question or the product they are interested in.'}\n",
      "{'role': 'user', 'content': 'Chị muốn hỏi về chính sách bảo hành pin điện thoại mua ở shop mình.'}\n",
      "{'role': 'assistant', 'content': 'Dạ chào chị. Pin điện thoại mua tại FPT Shop được bảo hành theo chính sách của hãng, từ 1 đến 12 tháng cho lỗi từ nhà sản xuất ạ. Nếu pin bị chai phồng trong thời gian bảo hành và không phải do lỗi người dùng, bên em sẽ hỗ trợ kiểm tra và đổi mới theo quy định. Để có thể kiểm tra thông tin bảo hành cụ thể cho sản phẩm hoặc tư vấn chi tiết hơn, chị có thể cho em xin số điện thoại hoặc email được không ạ?'}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "\n",
    "data_path = train_tone_file_path\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "'''\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    dataset = [line for line in reader]\n",
    "'''\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 50\n",
      "First example:\n",
      "{'role': 'system', 'content': \"## RULES:\\n- Use only the Vietnamese language in your response. Always refer to yourself using 'em' pronoun. Address the user based on how they refer to themselves . If their preferred address term cannot be determined from their self-reference, then base it on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"}\n",
      "{'role': 'system', 'content': '## BASE KNOWLEDGE:\\n- User gender: female'}\n",
      "{'role': 'system', 'content': '## INSTRUCTIONS:\\n You must ask the user for their contact information to provide better advice on their issue or policy question or the product they are interested in.'}\n",
      "{'role': 'user', 'content': 'Chị muốn hỏi về chính sách bảo hành pin điện thoại mua ở shop mình.'}\n",
      "{'role': 'assistant', 'content': 'Dạ chào chị. Pin điện thoại mua tại FPT Shop được bảo hành theo chính sách của hãng, từ 1 đến 12 tháng cho lỗi từ nhà sản xuất ạ. Nếu pin bị chai phồng trong thời gian bảo hành và không phải do lỗi người dùng, bên em sẽ hỗ trợ kiểm tra và đổi mới theo quy định. Để có thể kiểm tra thông tin bảo hành cụ thể cho sản phẩm hoặc tư vấn chi tiết hơn, chị có thể cho em xin số điện thoại hoặc email được không ạ?'}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "data_path = \"fine-tuning_data/train_tone_ds1.jsonl\"\n",
    "# Load the dataset\n",
    "\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    dataset = [line for line in reader]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 4, 6\n",
      "mean / median: 4.64, 5.0\n",
      "p5 / p95: 4.0, 5.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 182, 753\n",
      "mean / median: 334.2, 309.0\n",
      "p5 / p95: 201.9, 434.40000000000003\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 38, 281\n",
      "mean / median: 134.76, 128.0\n",
      "p5 / p95: 66.5, 207.5\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~16710 tokens that will be charged for during training\n",
      "By default, you'll train for 5 epochs on this dataset\n",
      "By default, you'll be charged for ~83550 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 5\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# import fix_path\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from service.openai import _client, _chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-XTfeJeiVUapJ8ABxnhfW9C\n"
     ]
    }
   ],
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = _client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id\n",
    "\n",
    "training_file_id = upload_file(train_tone_file_path, \"fine-tune\") #!openai files create -p fine-tune -f dataset.jsonl\n",
    "'''\n",
    "In addition to training data, we can also optionally provide validation data, which will be used to make sure that the model does not overfit your training set.\n",
    "'''\n",
    "print(\"Training file ID:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!openai fine_tuning.jobs.create -m gpt-4o -t <file-id>\n",
    "def create_fine_tuning_job(training_file_id, chatmodel,n_epochs,learning_rate):\n",
    "  response = _client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    #validation_file=validation_file_id,\n",
    "    model=chatmodel,\n",
    "    integrations=[\n",
    "        {\n",
    "          \"type\": \"wandb\",\n",
    "          \"wandb\": {\n",
    "              \"project\": \"CHATBOT-TLCN\",\n",
    "              \"name\": \"gpt-4o-mini-fine-tuning-tone\",\n",
    "              \"tags\": [\"openai/finetune\"]\n",
    "          }\n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    method={\n",
    "      \"type\": \"supervised\",\n",
    "      'supervised':{\n",
    "        'hyperparameters':{\n",
    "          'n_epochs': n_epochs, # auto=3 , one full cycle through train set\n",
    "          'learning_rate_multiplier': learning_rate,#Scaling factor for the learning rate. smaller lr to avoid overfitting\n",
    "          'batch_size': \"auto\", #1 num exams/batch\n",
    "        }\n",
    "      }\n",
    "\n",
    "    }\n",
    "  )\n",
    "  return response\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check job status\n",
    "def check_job_status(job_id):\n",
    "  response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_job_status(job_id):\n",
    "    response = _client.fine_tuning.jobs.list_events(job_id)\n",
    "\n",
    "    events = response.data\n",
    "    events.reverse()\n",
    "    print(f\"---Monitor {job_id}---\")\n",
    "    for event in events:\n",
    "        print(event.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fine-tuned model id\n",
    "def get_fine_tuned_model_id(job_id):\n",
    "\n",
    "    response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "    fine_tuned_model_id = response.fine_tuned_model\n",
    "\n",
    "    if fine_tuned_model_id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n",
    "        )\n",
    "    print(\"Fine-tuned model ID:\", fine_tuned_model_id)\n",
    "    return fine_tuned_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-3MkBKnw860Z5EFS9tqloYVNx\n",
      "Job ID: ftjob-5tntFVtOjbsCLIqjCW2dABvg\n",
      "Job ID: ftjob-gW9fFrEThhAwUt7gZRmRW8Rw\n"
     ]
    }
   ],
   "source": [
    "#lr_lst = [\"auto\", 0.5, 2.5] # auto = 1.8 giá trị auto tự động điều chỉnh dựa trên kích thước batch size\n",
    "lr_lst = [\"auto\", 0.9, 2.7] # auto = 1.8 giá trị auto tự động điều chỉnh dựa trên kích thước batch size\n",
    "\n",
    "chatmodel = _chat_model\n",
    "n_epochs = 3\n",
    "job_id_lst = []\n",
    "for lr in lr_lst:\n",
    "    response_ft = create_fine_tuning_job(training_file_id, chatmodel,n_epochs,lr)\n",
    "    job_id = response_ft.id\n",
    "    print(\"Job ID:\", job_id)\n",
    "    job_id_lst.append(job_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: succeeded\n",
      "Trained Tokens: 39786\n",
      "Status: succeeded\n",
      "Trained Tokens: 39786\n",
      "Status: succeeded\n",
      "Trained Tokens: 39786\n"
     ]
    }
   ],
   "source": [
    "for job_id in job_id_lst:\n",
    "    response_stt = check_job_status(job_id)\n",
    "    print(\"Status:\", response_stt.status)\n",
    "    print(\"Trained Tokens:\", response_stt.trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::BePMJcc3\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::BePMzLnB\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::BePLas4R\n"
     ]
    }
   ],
   "source": [
    "# monitor\n",
    "fine_tuned_model_id_lst = []\n",
    "for job_id in job_id_lst:\n",
    "    #monitor_job_status(job_id)\n",
    "    fine_tuned_model_id_lst.append(get_fine_tuned_model_id(job_id))\n",
    "\n",
    "'''save result to a txt file'''\n",
    "with open(\"fine-tuned_model_id.txt\", \"w\") as f:\n",
    "    for fine_tuned_model_id in fine_tuned_model_id_lst:\n",
    "        f.write(fine_tuned_model_id + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:personal::BeMUcpFU\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BeMUjDNW\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BeMUcwhA\n"
     ]
    }
   ],
   "source": [
    "for fine_tuned_model_id in fine_tuned_model_id_lst:\n",
    "    print(fine_tuned_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:personal::BePMJcc3\n",
      "0.12726236879825592\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePMJHNg:ckpt-step-100\n",
      "0.6972470879554749\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePMJXDq:ckpt-step-50\n",
      "1.0708763599395752\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePMzLnB\n",
      "0.45584574341773987\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePMzMbT:ckpt-step-100\n",
      "0.551426112651825\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePMyK6B:ckpt-step-50\n",
      "1.3087761402130127\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePLas4R\n",
      "0.2537245452404022\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePLaKdq:ckpt-step-100\n",
      "0.3635803461074829\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BePLaZa5:ckpt-step-50\n",
      "0.9651479721069336\n"
     ]
    }
   ],
   "source": [
    "for job_id in job_id_lst:\n",
    "    response_cp = _client.fine_tuning.jobs.checkpoints.list(job_id)\n",
    "    for checkpoint in response_cp.data:\n",
    "        #print(checkpoint.id)\n",
    "        print(checkpoint.fine_tuned_model_checkpoint)\n",
    "        print(checkpoint.metrics.train_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Could not find fine tune: ft:gpt-4o-mini-2024-07-18:personal::BeMUcpFU', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'fine_tune_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#get all checkpoint of each fine-tuned model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fine_tuned_model_id \u001b[38;5;129;01min\u001b[39;00m fine_tuned_model_id_lst:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#response = _client.fine_tuning.jobs.list_checkpoints(fine_tuned_model_id)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     response_cp \u001b[38;5;241m=\u001b[39m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuned_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m response_cp\u001b[38;5;241m.\u001b[39mdata:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(checkpoint\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[0;32m~/chatbot-tlcn/.venv/lib/python3.12/site-packages/openai/resources/fine_tuning/jobs/checkpoints.py:75\u001b[0m, in \u001b[0;36mCheckpoints.list\u001b[0;34m(self, fine_tuning_job_id, after, limit, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fine_tuning_job_id:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `fine_tuning_job_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuning_job_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fine_tuning/jobs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfine_tuning_job_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSyncCursorPage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFineTuningJobCheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mafter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlimit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_list_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointListParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFineTuningJobCheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chatbot-tlcn/.venv/lib/python3.12/site-packages/openai/_base_client.py:1288\u001b[0m, in \u001b[0;36mSyncAPIClient.get_api_list\u001b[0;34m(self, path, model, page, body, options, method)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_api_list\u001b[39m(\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1279\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncPageT:\n\u001b[1;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m-> 1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_api_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chatbot-tlcn/.venv/lib/python3.12/site-packages/openai/_base_client.py:1139\u001b[0m, in \u001b[0;36mSyncAPIClient._request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[1;32m   1137\u001b[0m options\u001b[38;5;241m.\u001b[39mpost_parser \u001b[38;5;241m=\u001b[39m _parser\n\u001b[0;32m-> 1139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chatbot-tlcn/.venv/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Could not find fine tune: ft:gpt-4o-mini-2024-07-18:personal::BeMUcpFU', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'fine_tune_not_found'}}"
     ]
    }
   ],
   "source": [
    "#get all checkpoint of each fine-tuned model\n",
    "fine_tuned_model_cp_lst = [{}]\n",
    "for fine_tuned_model_id in fine_tuned_model_id_lst:\n",
    "    #response = _client.fine_tuning.jobs.list_checkpoints(fine_tuned_model_id)\n",
    "    response_cp = _client.fine_tuning.jobs.checkpoints.list(fine_tuned_model_id)\n",
    "    for checkpoint in response_cp.data:\n",
    "        print(checkpoint.id)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
