{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " khi dùng của OpenAI để tinh chỉnh (fine-tune) các mô hình như GPT-3.5-Turbo, GPT-4o, GPT-4o-mini, không thể chọn các phương pháp tinh chỉnh tham số hiệu quả (Parameter-Efficient Fine-Tuning - PEFT) cụ thể như LoRA (Low-Rank Adaptation).\n",
    "\n",
    " Quá trình tinh chỉnh thông qua API của OpenAI là 1 (managed service). Bằng cách cung cấp training data (theo định dạng Chat Completions hoặc DPO) và một số siêu tham số cơ bản (như số epochs), còn OpenAI sẽ xử lý toàn bộ quá trình huấn luyện phức tạp ở phía sau. Kết quả nhận được một định danh mô hình mới đã được tinh chỉnh.\n",
    "\n",
    "\n",
    "API của OpenAI: Chỉ hỗ trợ các phương pháp tinh chỉnh được mô tả trong docs (chủ yếu là SFT và DPO theo định dạng quy định). Người dùng không có quyền kiểm soát trực tiếp kiến trúc hoặc phương pháp cập nhật trọng số ở mức độ thấp như LoRA. OpenAI trừu tượng hóa quá trình này đi. Việc họ có sử dụng các kỹ thuật PEFT nào đó ở bên trong hệ thống của họ để tối ưu hóa hay không thì không được công bố rõ ràng, nhưng người dùng không thể yêu cầu hay cấu hình nó.\n",
    "\n",
    "Tinh chỉnh Mô hình Open Source: Ngược lại, với các Opensource LLM (như Llama, Mistral, v.v.) trên các nền tảng như Hugging Face, hoàn toàn có thể áp dụng các kỹ thuật PEFT như LoRA, QLoRA, Adapter Tuning, v.v. Vì có quyền truy cập trực tiếp vào trọng số của mô hình và có thể tự mình kiểm soát toàn bộ vòng lặp huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example format\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "```\n",
    "\n",
    "Multi-turn chat examples\n",
    "\n",
    " The default behavior during fine-tuning is to train on all assistant messages within a single example. To skip fine-tuning on specific assistant messages, a weight key can be added disable fine-tuning on that message, allowing you to control which assistant messages are learned. The allowed values for weight are currently 0 or 1. Some examples using weight for the chat format are below.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\", \"weight\": 0}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\", \"weight\": 1}]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFT \n",
    "\n",
    "Nếu không chỉ định (method), mặc định là (Supervised Fine-Tuning - SFT).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Direct Preference Optimization - DPO](https://arxiv.org/abs/2305.18290)\n",
    "\n",
    "tinh chỉnh các mô hình dựa trên lời nhắc và các cặp phản hồi. Cách tiếp cận này cho phép mô hình học hỏi từ các ưu tiên của con người, tối ưu hóa cho các đầu ra có nhiều khả năng được ưa chuộng hơn\n",
    "\n",
    "For DPO:\n",
    "\n",
    "set the `type` parameter to `dpo`\n",
    "optionally set the `hyperparameters` property with any options you'd like to configure\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  \"input\": {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, can you tell me how cold San Francisco is today?\"\n",
    "      }\n",
    "    ],\n",
    "    \"tools\": [],\n",
    "    \"parallel_tool_calls\": true\n",
    "  },\n",
    "  \"preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Today in San Francisco, it is not quite cold as expected. Morning clouds will give away to sunshine, with a high near 68°F (20°C) and a low around 57°F (14°C).\"\n",
    "    }\n",
    "  ],\n",
    "  \"non_preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"It is not particularly cold in San Francisco today.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-all-about-the-weather\",\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.1},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and analysis cost for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs. The method shown here corresponds to the current fine-tuning method for gpt-3.5-turbo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid JSONL file\n",
      "Valid JSONL file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttps://jsonltools.com/jsonl-validator\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tone_file_path = \"fine-tuning_data/train_tone_ds1.jsonl\"\n",
    "validation_tone_file_path = \"fine-tuning_data/validation_tone_ds.jsonl\"\n",
    "def validate_jsonl(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                json.loads(line)\n",
    "        print(\"Valid JSONL file\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Invalid JSONL: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "\n",
    "validate_jsonl(train_tone_file_path)\n",
    "validate_jsonl(validation_tone_file_path)\n",
    "\n",
    "'''\n",
    "https://jsonltools.com/jsonl-validator\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading\n",
    "We first load the chat dataset from an example JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 60\n",
      "First example:\n",
      "{'role': 'system', 'content': \"## RULES:\\n- Use only the Vietnamese language in your response and base the form of address on the 'User gender' in <BASE KNOWLEDGE> and user messages to ensure appropriate pronoun usage.\"}\n",
      "{'role': 'system', 'content': '## BASE KNOWLEDGE:\\n- User gender: female'}\n",
      "{'role': 'system', 'content': '## INSTRUCTIONS:\\n You must ask the user for their contact information to provide better advice on their issue or policy question or the product they are interested in.'}\n",
      "{'role': 'user', 'content': 'Chị muốn hỏi về chính sách bảo hành pin điện thoại mua ở shop mình.'}\n",
      "{'role': 'assistant', 'content': 'Dạ chào chị. Pin điện thoại mua tại FPT Shop được bảo hành theo chính sách của hãng, từ 1 đến 12 tháng cho lỗi từ nhà sản xuất ạ. Nếu pin bị chai phồng trong thời gian bảo hành và không phải do lỗi người dùng, bên em sẽ hỗ trợ kiểm tra và đổi mới theo quy định. Để có thể kiểm tra thông tin bảo hành cụ thể cho sản phẩm hoặc tư vấn chi tiết hơn, chị có thể cho em xin số điện thoại hoặc email được không ạ?'}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "#data_path = \"fine-tuning_data/tone1.jsonl\"\n",
    "#data_path = \"fine-tuning_data/toy_chat_fine_tuning.jsonl\"\n",
    "# Load the dataset\n",
    "data_path = train_tone_file_path\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "'''\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    dataset = [line for line in reader]\n",
    "'''\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 50\n",
      "First example:\n",
      "{'role': 'system', 'content': \"## RULES:\\n- Use only the Vietnamese language in your response. Always refer to yourself using 'em' pronoun. Address the user based on how they refer to themselves . If their preferred address term cannot be determined from their self-reference, then base it on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"}\n",
      "{'role': 'system', 'content': '## BASE KNOWLEDGE:\\n- User gender: female'}\n",
      "{'role': 'system', 'content': '## INSTRUCTIONS:\\n You must ask the user for their contact information to provide better advice on their issue or policy question or the product they are interested in.'}\n",
      "{'role': 'user', 'content': 'Chị muốn hỏi về chính sách bảo hành pin điện thoại mua ở shop mình.'}\n",
      "{'role': 'assistant', 'content': 'Dạ chào chị. Pin điện thoại mua tại FPT Shop được bảo hành theo chính sách của hãng, từ 1 đến 12 tháng cho lỗi từ nhà sản xuất ạ. Nếu pin bị chai phồng trong thời gian bảo hành và không phải do lỗi người dùng, bên em sẽ hỗ trợ kiểm tra và đổi mới theo quy định. Để có thể kiểm tra thông tin bảo hành cụ thể cho sản phẩm hoặc tư vấn chi tiết hơn, chị có thể cho em xin số điện thoại hoặc email được không ạ?'}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "data_path = \"fine-tuning_data/train_tone_ds1.jsonl\"\n",
    "#data_path = \"fine-tuning_data/toy_chat_fine_tuning.jsonl\"\n",
    "# Load the dataset\n",
    "\n",
    "'''\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "'''\n",
    "with jsonlines.open(data_path) as reader:\n",
    "    dataset = [line for line in reader]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. Data Type Check: Checks whether each entry in the dataset is a dictionary (dict). Error type: data_type.\n",
    "2. Presence of Message List: Checks if a messages list is present in each entry. Error type: missing_messages_list.\n",
    "3. Message Keys Check: Validates that each message in the messages list contains the keys role and content. Error type: message_missing_key.\n",
    "4. Unrecognized Keys in Messages: Logs if a message has keys other than role, content, weight, function_call, and name. Error type: message_unrecognized_key.\n",
    "5. Role Validation: Ensures the role is one of \"system\", \"user\", or \"assistant\". Error type: unrecognized_role.\n",
    "6. Content Validation: Verifies that content has textual data and is a string. Error type: missing_content.\n",
    "7. Assistant Message Presence: Checks that each conversation has at least one message from the assistant. Error type: example_missing_assistant_message.\n",
    "\n",
    "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Counting Utilities\n",
    "\n",
    "Lets define a few helpful utilities to be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Warnings and Token Counts\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. Missing System/User Messages: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. Number of Messages Per Example: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. Total Tokens Per Example: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. Tokens in Assistant's Messages: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. Token Limit Warnings: Checks if any examples exceed the maximum token limit (16,385 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 4, 6\n",
      "mean / median: 4.64, 5.0\n",
      "p5 / p95: 4.0, 5.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 167, 738\n",
      "mean / median: 319.2, 294.0\n",
      "p5 / p95: 186.9, 419.40000000000003\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 38, 281\n",
      "mean / median: 134.76, 128.0\n",
      "p5 / p95: 66.5, 207.5\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "Estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~15960 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~47880 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://openai.com/pricing to estimate total costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# import fix_path\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import error because service module is not in Python path\n",
    "# Need to add parent directory to Python path first\n",
    "from service.openai import _client, _chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TÀI LIỆU THAM KHẢO\n",
    "- [OpenAI Fine-tuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload dataset.jsonl to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-TGkYkSQkCxFezpvwrmtfdh\n"
     ]
    }
   ],
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = _client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id\n",
    "\n",
    "\n",
    "\n",
    "#validation_file_id = upload_file(validation_tone_file_path, \"fine-tune\")\n",
    "training_file_id = upload_file(train_tone_file_path, \"fine-tune\") #!openai files create -p fine-tune -f dataset.jsonl\n",
    "\n",
    "'''\n",
    "In addition to training data, we can also optionally provide validation data, which will be used to make sure that the model does not overfit your training set.\n",
    "'''\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "\n",
    "#print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả trả về sẽ có file-id (ghi lại để dùng bước sau)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create fine-tuning job\n",
    "\n",
    "Now we can create our fine-tuning job with the generated files and an optional suffix to identify the model. The response will contain an id which you can use to retrieve updates on the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!openai fine_tuning.jobs.create -m gpt-4o -t <file-id>\n",
    "def create_fine_tuning_job(training_file_id, chatmodel,n_epochs,learning_rate):\n",
    "  response = _client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    #validation_file=validation_file_id,\n",
    "    model=chatmodel,\n",
    "    integrations=[\n",
    "        {\n",
    "          \"type\": \"wandb\",\n",
    "          \"wandb\": {\n",
    "              \"project\": \"CHATBOT-TLCN\",\n",
    "              \"name\": \"gpt-4o-mini-fine-tuning-tone\",\n",
    "              \"tags\": [\"openai/finetune\"]\n",
    "          }\n",
    "            \n",
    "        }\n",
    "    ],\n",
    "    method={\n",
    "      \"type\": \"supervised\",\n",
    "      'supervised':{\n",
    "        'hyperparameters':{\n",
    "          'n_epochs': n_epochs, # auto=3 , one full cycle through train set\n",
    "          'learning_rate_multiplier': learning_rate,#Scaling factor for the learning rate. smaller lr to avoid overfitting\n",
    "          'batch_size': \"auto\", #1 num exams/batch\n",
    "        }\n",
    "      }\n",
    "\n",
    "    }\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check job status\n",
    "def check_job_status(job_id):\n",
    "  response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_job_status(job_id):\n",
    "    response = _client.fine_tuning.jobs.list_events(job_id)\n",
    "\n",
    "    events = response.data\n",
    "    events.reverse()\n",
    "    print(f\"---Monitor {job_id}---\")\n",
    "    for event in events:\n",
    "        print(event.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fine-tuned model id\n",
    "def get_fine_tuned_model_id(job_id):\n",
    "\n",
    "    response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "    fine_tuned_model_id = response.fine_tuned_model\n",
    "\n",
    "    if fine_tuned_model_id is None:\n",
    "        raise RuntimeError(\n",
    "            \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n",
    "        )\n",
    "    print(\"Fine-tuned model ID:\", fine_tuned_model_id)\n",
    "    return fine_tuned_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-M2qbbRtobmLxdaDCqyZ8jAoP\n"
     ]
    }
   ],
   "source": [
    "chatmodel = _chat_model\n",
    "n_epochs = 3\n",
    "lr_lst = [\"auto\"] # auto = 1.8 giá trị auto tự động điều chỉnh dựa trên kích thước batch size\n",
    "job_id_lst = []\n",
    "for lr in lr_lst:\n",
    "    response_ft = create_fine_tuning_job(training_file_id, chatmodel,n_epochs,lr)\n",
    "    job_id = response_ft.id\n",
    "    print(\"Job ID:\", job_id)\n",
    "    job_id_lst.append(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: running\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "for job_id in job_id_lst:\n",
    "    response_stt = check_job_status(job_id)\n",
    "    print(\"Status:\", response_stt.status)\n",
    "    print(\"Trained Tokens:\", response_stt.trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::BejsaPFu\n"
     ]
    }
   ],
   "source": [
    "# monitor\n",
    "fine_tuned_model_id_lst = []\n",
    "for job_id in job_id_lst:\n",
    "    #monitor_job_status(job_id)\n",
    "    fine_tuned_model_id_lst.append(get_fine_tuned_model_id(job_id))\n",
    "\n",
    "'''save result to a txt file'''\n",
    "with open(\"fine-tuned_model_id.txt\", \"w\") as f:\n",
    "    for fine_tuned_model_id in fine_tuned_model_id_lst:\n",
    "        f.write(fine_tuned_model_id + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:personal::BejsaPFu\n"
     ]
    }
   ],
   "source": [
    "for fine_tuned_model_id in fine_tuned_model_id_lst:\n",
    "    print(fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:personal::BejsaPFu\n",
      "0.11896969377994537\n",
      "ft:gpt-4o-mini-2024-07-18:personal::Bejsa6J7:ckpt-step-120\n",
      "1.0175371170043945\n",
      "ft:gpt-4o-mini-2024-07-18:personal::BejsZy9k:ckpt-step-60\n",
      "0.913822591304779\n"
     ]
    }
   ],
   "source": [
    "for job_id in job_id_lst:\n",
    "    response_cp = _client.fine_tuning.jobs.checkpoints.list(job_id)\n",
    "    for checkpoint in response_cp.data:\n",
    "        #print(checkpoint.id)\n",
    "        print(checkpoint.fine_tuned_model_checkpoint)\n",
    "        print(checkpoint.metrics.train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check job status\n",
    "\n",
    "You can make a GET request to the https://api.openai.com/v1/alpha/fine-tunes endpoint to list your alpha fine-tune jobs. In this instance you'll want to check that the ID you got from the previous step ends up as status: succeeded.\n",
    "\n",
    "Once it is completed, you can use the result_files to sample the results from the validation set (if you uploaded one), and use the ID from the fine_tuned_model parameter to invoke your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-cNJXOSSfAvfvBcJ0Ww3ZVjvF\n",
      "Status: succeeded\n",
      "Trained Tokens: 39786\n"
     ]
    }
   ],
   "source": [
    "job_id = \"ftjob-cNJXOSSfAvfvBcJ0Ww3ZVjvF\"\n",
    "response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "#!openai fine_tuning.jobs.list\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the progress of the fine-tune with the events endpoint. You can rerun the cell below a few times until the fine-tune is ready.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 138/150: training loss=0.62\n",
      "Step 139/150: training loss=0.98\n",
      "Step 140/150: training loss=0.33\n",
      "Step 141/150: training loss=0.40\n",
      "Step 142/150: training loss=0.30\n",
      "Step 143/150: training loss=0.38\n",
      "Step 144/150: training loss=0.87\n",
      "Step 145/150: training loss=0.24\n",
      "Step 146/150: training loss=0.13\n",
      "Step 147/150: training loss=0.55\n",
      "Step 148/150: training loss=0.63\n",
      "Step 149/150: training loss=0.21\n",
      "Step 150/150: training loss=0.08\n",
      "Checkpoint created at step 50\n",
      "Checkpoint created at step 100\n",
      "New fine-tuned model created\n",
      "Evaluating model against our usage policies before enabling\n",
      "Moderation checks for snapshot ft:gpt-4o-mini-2024-07-18:personal::Bdao7Wp2 passed\n",
      "Usage policy evaluations completed, model is now enabled for sampling\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "response = _client.fine_tuning.jobs.list_events(job_id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's done, we can get a fine-tuned model ID from the job:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::Bdao7Wp2\n"
     ]
    }
   ],
   "source": [
    "response = _client.fine_tuning.jobs.retrieve(job_id)\n",
    "fine_tuned_model_id = response.fine_tuned_model\n",
    "\n",
    "if fine_tuned_model_id is None:\n",
    "    raise RuntimeError(\n",
    "        \"Fine-tuned model ID not found. Your job has likely not been completed yet.\"\n",
    "    )\n",
    "\n",
    "print(\"Fine-tuned model ID:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 138/150: training loss=0.62\n",
      "Step 139/150: training loss=0.98\n",
      "Step 140/150: training loss=0.33\n",
      "Step 141/150: training loss=0.40\n",
      "Step 142/150: training loss=0.30\n",
      "Step 143/150: training loss=0.38\n",
      "Step 144/150: training loss=0.87\n",
      "Step 145/150: training loss=0.24\n",
      "Step 146/150: training loss=0.13\n",
      "Step 147/150: training loss=0.55\n",
      "Step 148/150: training loss=0.63\n",
      "Step 149/150: training loss=0.21\n",
      "Step 150/150: training loss=0.08\n",
      "Checkpoint created at step 50\n",
      "Checkpoint created at step 100\n",
      "New fine-tuned model created\n",
      "Evaluating model against our usage policies before enabling\n",
      "Moderation checks for snapshot ft:gpt-4o-mini-2024-07-18:personal::Bdao7Wp2 passed\n",
      "Usage policy evaluations completed, model is now enabled for sampling\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "response = _client.fine_tuning.jobs.list_events(job_id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using model fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dạ chào chị. Chị đang quan tâm đến model iPhone 15 cụ thể nào ạ? FPT Shop hiện có nhiều phiên bản iPhone 15 với các mức giá và ưu đãi khác nhau. Em có thể tư vấn thông tin chi tiết về sản phẩm hoặc chương trình khuyến mãi đang áp dụng ạ.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = _client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional, polite, patient, and respectful sales consultant staff for a phone store, interacting with Vietnamese customers. Always refer to yourself using 'em' pronoun. Address the user based on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"},\n",
    "    {\"role\": \"system\", \"content\": \"## BASE KNOWLEDGE:\\n- User gender: female\"}, # Cung cấp ngữ cảnh giới tính\n",
    "    {\"role\": \"user\", \"content\": \"Chào shop, em muốn hỏi về điện thoại Iphone 15\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dạ chào anh/chị, Anh/chị quan tâm đến iPhone 15 phiên bản, màu sắc, giá cả hay chương trình khuyến mãi cụ thể nào ạ? Anh/chị có thể cho em biết để em tư vấn chi tiết hơn ạ.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = _client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional, polite, patient, and respectful sales consultant staff for a phone store, interacting with Vietnamese customers. Always refer to yourself using 'em' pronoun. Address the user based on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"},\n",
    "    {\"role\": \"system\", \"content\": \"## BASE KNOWLEDGE:\\n- User gender: unknown\"}, # Cung cấp ngữ cảnh giới tính\n",
    "    {\"role\": \"user\", \"content\": \"Chào shop, em muốn hỏi về điện thoại Iphone 15\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dạ chào anh. Anh cần tư vấn về dòng iPhone 15 ạ, cụ thể là model, giá cả hay thời gian bảo hành ạ? Anh cũng có thể cho em biết anh đang quan tâm đến phiên bản nào (iPhone 15, 15 Plus, 15 Pro, hay 15 Pro Max) không ạ? Mọi thông tin em sẽ hỗ trợ anh nhanh chóng.\n"
     ]
    }
   ],
   "source": [
    "completion = _client.chat.completions.create(\n",
    "  model=fine_tuned_model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional, polite, patient, and respectful sales consultant staff for a phone store, interacting with Vietnamese customers. Always refer to yourself using 'em' pronoun. Address the user based on their provided <User gender>: use 'anh' for male, 'chị' for female. If the gender is unknown or not provided, use the polite neutral term 'anh/chị'.\"},\n",
    "    {\"role\": \"system\", \"content\": \"## BASE KNOWLEDGE:\\n- User gender: unknown\"}, # Cung cấp ngữ cảnh giới tính\n",
    "    {\"role\": \"user\", \"content\": \"Anh muốn hỏi về điện thoại Iphone 15\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'client_secret_1046238123460-9tjfjrdougg0t1kj3i53i3itqt67gqd9.apps.googleusercontent.com.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrepositories\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create \u001b[38;5;28;01mas\u001b[39;00m create_user, CreateUserModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrepositories\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthread\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create \u001b[38;5;28;01mas\u001b[39;00m create_thread, CreateThreadModel\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstore_chatbot_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_answer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserRole    \n",
      "File \u001b[0;32m~/chatbot-tlcn/service/store_chatbot_v2.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccessory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_response\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccessories_generate_response\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mundetermined\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_response\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mundetermined_generate_response\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetect_demand\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdetect_demand\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m instructions_to_string\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m client \u001b[38;5;28;01mas\u001b[39;00m wandb_client\n",
      "File \u001b[0;32m~/chatbot-tlcn/agents/detect_demand.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Queue\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mweave\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_response_by_instructions\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m redis\n",
      "File \u001b[0;32m~/chatbot-tlcn/agents/utils.py:17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m redis\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     convert_to_standard_email,\n\u001b[1;32m     15\u001b[0m     convert_to_standard_phone_number,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01memail\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_message, send_message\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIChatCompletionsRequest, _client, _chat_model\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatModel\n",
      "File \u001b[0;32m~/chatbot-tlcn/service/email.py:47\u001b[0m\n\u001b[1;32m     42\u001b[0m     raw \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39murlsafe_b64encode(message\u001b[38;5;241m.\u001b[39mas_bytes())\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_message\u001b[39m(\n\u001b[0;32m---> 47\u001b[0m     message: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m, service\u001b[38;5;241m=\u001b[39m\u001b[43m_authenticate_gmail\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, user_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m ):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send an email using the Gmail API.\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/chatbot-tlcn/service/email.py:26\u001b[0m, in \u001b[0;36m_authenticate_gmail\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     creds\u001b[38;5;241m.\u001b[39mrefresh(Request())\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     flow \u001b[38;5;241m=\u001b[39m \u001b[43mInstalledAppFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_client_secrets_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGOOGLE_APPLICATION_CREDENTIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCOPES\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     creds \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39mrun_local_server(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Save credentials for future use\u001b[39;00m\n",
      "File \u001b[0;32m~/chatbot-tlcn/.venv/lib/python3.12/site-packages/google_auth_oauthlib/flow.py:198\u001b[0m, in \u001b[0;36mFlow.from_client_secrets_file\u001b[0;34m(cls, client_secrets_file, scopes, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_client_secrets_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, client_secrets_file, scopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a :class:`Flow` instance from a Google client secrets file.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        Flow: The constructed Flow instance.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclient_secrets_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m    199\u001b[0m         client_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_client_config(client_config, scopes\u001b[38;5;241m=\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secret_1046238123460-9tjfjrdougg0t1kj3i53i3itqt67gqd9.apps.googleusercontent.com.json'"
     ]
    }
   ],
   "source": [
    "from repositories.user import create as create_user, CreateUserModel\n",
    "from repositories.thread import create as create_thread, CreateThreadModel\n",
    "from service.store_chatbot_v2 import gen_answer\n",
    "from uuid import uuid4\n",
    "from models.user import UserRole    \n",
    "\n",
    "def get_actual_answer(input: str, gender_context: str) -> str:\n",
    "    user = create_user(\n",
    "        CreateUserModel(user_name=str(uuid4()), role=UserRole.chainlit_user)\n",
    "    )\n",
    "    thread = create_thread(CreateThreadModel(user_id=user.id, name=user.user_name))\n",
    "    \n",
    "    # Add gender context to the conversation\n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": \"##BASE KNOWLEDGE:\\n\" + gender_context},\n",
    "        {\"role\": \"user\", \"content\": str(input)}\n",
    "    ]\n",
    "\n",
    "    return gen_answer(\n",
    "        thread_id=thread.id,\n",
    "        history=history,\n",
    "        user_id=user.id,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_actual_answer(\"Cô cần liên lạc với tổng đài của FPTShop.\", \"unknown\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
